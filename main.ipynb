{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ[\"SM_FRAMEWORK\"]=\"tf.keras\"\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "from data_classes import Dataloder, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './true_data/'\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train_data')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_mask')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val_data')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'val_mask')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test_data')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'test_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# helper function for data visualization    \n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'resnet18'\n",
    "BATCH_SIZE = 8\n",
    "LR = 0.0001\n",
    "EPOCHS = 40\n",
    "\n",
    "total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "model = sm.Unet(BACKBONE,input_shape = (512, 512, 3))\n",
    "# compile keras model with defined optimozer, loss and metrics\n",
    "model.compile(\"Adam\", total_loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define callbacks for learning rate scheduling and best checkpoints saving\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./best_model_true.h5', save_weights_only=True, save_best_only=True, mode='min')\n",
    "    #keras.callbacks.ReduceLROnPlateau(),\n",
    "]\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    ")\n",
    "\n",
    "# Dataset for validation images\n",
    "valid_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    ")\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataloader,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=EPOCHS, \n",
    "    callbacks=callbacks,\n",
    "    validation_data = valid_dataloader,\n",
    "    validation_steps=len(valid_dataloader),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['iou_score'])\n",
    "plt.plot(history.history['val_iou_score'])\n",
    "plt.title('Model iou_score')\n",
    "plt.ylabel('iou_score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir\n",
    ")\n",
    "\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# load best weights\n",
    "model.load_weights('best_model_true.h5')\n",
    "\n",
    "scores = model.evaluate_generator(test_dataloader)\n",
    "\n",
    "print(\"Loss: {:.5}\".format(scores[0]))\n",
    "for metric, value in zip(metrics, scores[1:]):\n",
    "    print(\"mean {}: {:.5}\".format(metric.__name__, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "ids = np.random.choice(np.arange(len(test_dataset)), size=n)\n",
    "\n",
    "for i in ids:\n",
    "    \n",
    "    image, gt_mask = test_dataset[i]\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    pr_mask = model.predict(image).round()\n",
    "    \n",
    "    visualize(\n",
    "        image=denormalize(image.squeeze()),\n",
    "        gt_mask=gt_mask.squeeze(),\n",
    "        pr_mask=pr_mask.squeeze(),\n",
    "    )"
   ]
  },
  {
   "source": [
    "# Полностью сгенерированные данные #"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './false_data/'\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train_data')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_mask')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks for learning rate scheduling and best checkpoints saving\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./best_model_false.h5', save_weights_only=True, save_best_only=True, mode='min')\n",
    "    #keras.callbacks.ReduceLROnPlateau(),\n",
    "]\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataloader,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=EPOCHS, \n",
    "    callbacks=callbacks,\n",
    "    validation_data = valid_dataloader,\n",
    "    validation_steps=len(valid_dataloader),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['iou_score'])\n",
    "plt.plot(history.history['val_iou_score'])\n",
    "plt.title('Model iou_score')\n",
    "plt.ylabel('iou_score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_false.h5')\n",
    "\n",
    "scores = model.evaluate_generator(test_dataloader)\n",
    "\n",
    "print(\"Loss: {:.5}\".format(scores[0]))\n",
    "for metric, value in zip(metrics, scores[1:]):\n",
    "    print(\"mean {}: {:.5}\".format(metric.__name__, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ids:\n",
    "    \n",
    "    image, gt_mask = test_dataset[i]\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    pr_mask = model.predict(image).round()\n",
    "    \n",
    "    visualize(\n",
    "        image=denormalize(image.squeeze()),\n",
    "        gt_mask=gt_mask.squeeze(),\n",
    "        pr_mask=pr_mask.squeeze(),\n",
    "    )"
   ]
  },
  {
   "source": [
    "# Смешанные данные #"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './mixed_data/'\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train_data')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_mask')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks for learning rate scheduling and best checkpoints saving\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./best_model_false.h5', save_weights_only=True, save_best_only=True, mode='min')\n",
    "    #keras.callbacks.ReduceLROnPlateau(),\n",
    "]\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataloader,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=EPOCHS, \n",
    "    callbacks=callbacks,\n",
    "    validation_data = valid_dataloader,\n",
    "    validation_steps=len(valid_dataloader),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['iou_score'])\n",
    "plt.plot(history.history['val_iou_score'])\n",
    "plt.title('Model iou_score')\n",
    "plt.ylabel('iou_score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_false.h5')\n",
    "\n",
    "scores = model.evaluate_generator(test_dataloader)\n",
    "\n",
    "print(\"Loss: {:.5}\".format(scores[0]))\n",
    "for metric, value in zip(metrics, scores[1:]):\n",
    "    print(\"mean {}: {:.5}\".format(metric.__name__, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ids:\n",
    "    \n",
    "    image, gt_mask = test_dataset[i]\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    pr_mask = model.predict(image).round()\n",
    "    \n",
    "    visualize(\n",
    "        image=denormalize(image.squeeze()),\n",
    "        gt_mask=gt_mask.squeeze(),\n",
    "        pr_mask=pr_mask.squeeze(),\n",
    "    )"
   ]
  }
 ]
}